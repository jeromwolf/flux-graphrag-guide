{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Entity Resolution — 중복 엔티티 통합\n",
    "\n",
    "**소요시간:** 1시간  \n",
    "**난이도:** ★★★☆  \n",
    "**마일스톤:** 정제된 KG — 중복 제거 완료 (예: 45개 → 30개 노드)\n",
    "\n",
    "---\n",
    "\n",
    "## 학습 목표\n",
    "\n",
    "1. **중복 엔티티**가 Knowledge Graph 품질에 미치는 영향 이해\n",
    "2. **문자열 유사도** 기반 Entity Resolution (rapidfuzz)\n",
    "3. **임베딩 기반** Entity Resolution (OpenAI Embeddings + 코사인 유사도)\n",
    "4. Neo4j에서 **노드 통합** (MERGE + 관계 재연결)\n",
    "5. 통합 전/후 **쿼리 품질 비교**"
   ],
   "id": "b1c2d3e4"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. 환경 설정\n",
    "\n",
    "Neo4j에 연결하고, Part 3에서 적재한 기존 그래프를 확인합니다."
   ],
   "id": "f5a6b7c8"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 필수 패키지 임포트\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from openai import OpenAI\n",
    "from neo4j import GraphDatabase\n",
    "from rapidfuzz import fuzz, process\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 한글 폰트 설정\n",
    "matplotlib.rcParams['font.family'] = 'AppleGothic'  # macOS\n",
    "# matplotlib.rcParams['font.family'] = 'Malgun Gothic'  # Windows\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"패키지 로드 완료\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "d9e0f1a2"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 환경변수 및 연결\n",
    "load_dotenv()\n",
    "\n",
    "# OpenAI (임베딩용)\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Neo4j 연결\n",
    "NEO4J_URI = os.getenv(\"NEO4J_URI\", \"bolt://localhost:7687\")\n",
    "NEO4J_USER = os.getenv(\"NEO4J_USER\", \"neo4j\")\n",
    "NEO4J_PASSWORD = os.getenv(\"NEO4J_PASSWORD\", \"graphrag2024\")\n",
    "\n",
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USER, NEO4J_PASSWORD))\n",
    "\n",
    "# 연결 테스트\n",
    "with driver.session() as session:\n",
    "    result = session.run(\"RETURN 1 AS test\")\n",
    "    print(f\"Neo4j 연결 성공: {result.single()['test']}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "b3c4d5e6"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 기존 그래프 상태 확인\n",
    "with driver.session() as session:\n",
    "    # 전체 노드/관계 수\n",
    "    node_count = session.run(\"MATCH (n) RETURN count(n) AS cnt\").single()[\"cnt\"]\n",
    "    rel_count = session.run(\"MATCH ()-[r]->() RETURN count(r) AS cnt\").single()[\"cnt\"]\n",
    "    \n",
    "    # 모든 엔티티 이름 조회\n",
    "    all_entities = session.run(\"\"\"\n",
    "        MATCH (n) \n",
    "        RETURN n.name AS name, labels(n) AS labels \n",
    "        ORDER BY n.name\n",
    "    \"\"\").data()\n",
    "\n",
    "print(f\"현재 그래프 상태:\")\n",
    "print(f\"  노드: {node_count}개\")\n",
    "print(f\"  관계: {rel_count}개\")\n",
    "print(f\"\\n전체 엔티티 목록 ({len(all_entities)}개):\")\n",
    "for e in all_entities:\n",
    "    print(f\"  - {e['name']} [{', '.join(e['labels'])}]\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "f7a8b9c0"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. 중복 문제 이해\n",
    "\n",
    "LLM이 추출한 엔티티에는 **같은 대상을 다르게 표현한 중복**이 빈번합니다.\n",
    "\n",
    "예시:\n",
    "- \"삼성전자\", \"Samsung Electronics\", \"Samsung\", \"삼성\"\n",
    "- \"SK하이닉스\", \"SK Hynix\", \"하이닉스\"\n",
    "- \"네이버\", \"NAVER\", \"Naver\"\n",
    "\n",
    "이런 중복은 **Multi-hop 쿼리의 정확도를 떨어뜨립니다.**"
   ],
   "id": "d1e2f3a4"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 중복 문제 시연을 위해 의도적으로 중복 엔티티를 추가\n",
    "# (Part 3에서 이미 LLM 추출 결과가 있다면 이 셀은 건너뛰어도 됩니다)\n",
    "\n",
    "duplicate_entities = [\n",
    "    # 삼성전자 변형\n",
    "    (\"Samsung Electronics\", \"Company\"),\n",
    "    (\"Samsung\", \"Company\"),\n",
    "    (\"삼성\", \"Company\"),\n",
    "    # SK하이닉스 변형\n",
    "    (\"SK Hynix\", \"Company\"),\n",
    "    (\"하이닉스\", \"Company\"),\n",
    "    # 네이버 변형\n",
    "    (\"NAVER\", \"Company\"),\n",
    "    (\"Naver\", \"Company\"),\n",
    "    # 엔비디아 변형\n",
    "    (\"NVIDIA\", \"Company\"),\n",
    "    (\"Nvidia\", \"Company\"),\n",
    "    # 하이퍼클로바 변형\n",
    "    (\"HyperCLOVA X\", \"Product\"),\n",
    "    (\"HyperClova X\", \"Product\"),\n",
    "]\n",
    "\n",
    "with driver.session() as session:\n",
    "    for name, label in duplicate_entities:\n",
    "        session.run(\n",
    "            f\"MERGE (n:{label} {{name: $name}})\",\n",
    "            name=name\n",
    "        )\n",
    "    \n",
    "    # 일부 중복 엔티티에 관계 추가 (문제를 더 현실적으로)\n",
    "    session.run(\"\"\"\n",
    "        MATCH (a:Company {name: \"Samsung Electronics\"})\n",
    "        MATCH (b:Company {name: \"NVIDIA\"})\n",
    "        MERGE (a)-[:PARTNERS_WITH]->(b)\n",
    "    \"\"\")\n",
    "    session.run(\"\"\"\n",
    "        MATCH (a:Company {name: \"NAVER\"})\n",
    "        MATCH (b:Product {name: \"HyperCLOVA X\"})\n",
    "        MERGE (a)-[:DEVELOPS]->(b)\n",
    "    \"\"\")\n",
    "    \n",
    "    # 변경 후 노드 수 확인\n",
    "    new_count = session.run(\"MATCH (n) RETURN count(n) AS cnt\").single()[\"cnt\"]\n",
    "\n",
    "print(f\"중복 엔티티 추가 완료\")\n",
    "print(f\"  이전 노드 수: {node_count}\")\n",
    "print(f\"  현재 노드 수: {new_count}\")\n",
    "print(f\"  추가된 중복: {new_count - node_count}개\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "b5c6d7e8"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 중복이 Multi-hop 쿼리에 미치는 영향 시연\n",
    "with driver.session() as session:\n",
    "    # 쿼리 1: \"삼성전자와 관련된 모든 엔티티\"\n",
    "    result1 = session.run(\"\"\"\n",
    "        MATCH (samsung:Company {name: \"삼성전자\"})-[r]-(connected)\n",
    "        RETURN samsung.name AS source, type(r) AS relation, connected.name AS target\n",
    "    \"\"\").data()\n",
    "    \n",
    "    # 쿼리 2: \"Samsung Electronics와 관련된 모든 엔티티\" (다른 결과!)\n",
    "    result2 = session.run(\"\"\"\n",
    "        MATCH (samsung:Company {name: \"Samsung Electronics\"})-[r]-(connected)\n",
    "        RETURN samsung.name AS source, type(r) AS relation, connected.name AS target\n",
    "    \"\"\").data()\n",
    "    \n",
    "    # 쿼리 3: 2-hop 연결 (삼성전자 → ? → ?)\n",
    "    result3 = session.run(\"\"\"\n",
    "        MATCH path = (samsung:Company {name: \"삼성전자\"})-[*1..2]-(target)\n",
    "        RETURN DISTINCT target.name AS reachable\n",
    "    \"\"\").data()\n",
    "\n",
    "print(\"중복이 쿼리에 미치는 영향:\")\n",
    "print(f\"\\n1) '삼성전자'로 검색한 관계: {len(result1)}개\")\n",
    "for r in result1:\n",
    "    print(f\"   {r['source']} --[{r['relation']}]--> {r['target']}\")\n",
    "\n",
    "print(f\"\\n2) 'Samsung Electronics'로 검색한 관계: {len(result2)}개\")\n",
    "for r in result2:\n",
    "    print(f\"   {r['source']} --[{r['relation']}]--> {r['target']}\")\n",
    "\n",
    "print(f\"\\n3) '삼성전자'에서 2-hop 도달 가능 엔티티: {len(result3)}개\")\n",
    "for r in result3:\n",
    "    print(f\"   - {r['reachable']}\")\n",
    "\n",
    "print(f\"\\n문제: 같은 기업인데 다른 이름으로 검색하면 다른 결과가 나옵니다!\")\n",
    "print(f\"Samsung Electronics의 관계가 삼성전자 검색에 포함되지 않습니다.\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "f9a0b1c2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. 문자열 유사도 기반 Entity Resolution\n",
    "\n",
    "**rapidfuzz** 라이브러리를 사용하여 이름이 비슷한 엔티티 쌍을 찾습니다.\n",
    "\n",
    "| 알고리즘 | 특징 | 적합한 경우 |\n",
    "|----------|------|------------|\n",
    "| Levenshtein | 편집 거리 기반 | 오타, 띄어쓰기 차이 |\n",
    "| Jaro-Winkler | 접두사 가중치 | 약어, 접두사 공유 |\n",
    "| Token Sort Ratio | 토큰 정렬 후 비교 | 단어 순서 다른 경우 |"
   ],
   "id": "d3e4f5a6"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Neo4j에서 모든 엔티티 이름 가져오기\n",
    "with driver.session() as session:\n",
    "    entities_data = session.run(\"\"\"\n",
    "        MATCH (n)\n",
    "        RETURN n.name AS name, labels(n) AS labels, id(n) AS node_id\n",
    "        ORDER BY n.name\n",
    "    \"\"\").data()\n",
    "\n",
    "entity_names = [e[\"name\"] for e in entities_data if e[\"name\"]]\n",
    "print(f\"총 엔티티: {len(entity_names)}개\")\n",
    "print(f\"\\n엔티티 목록:\")\n",
    "for name in sorted(entity_names):\n",
    "    print(f\"  - {name}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "b7c8d9e0"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 문자열 유사도 함수들 비교\n",
    "test_pairs = [\n",
    "    (\"삼성전자\", \"Samsung Electronics\"),\n",
    "    (\"삼성전자\", \"삼성\"),\n",
    "    (\"삼성전자\", \"Samsung\"),\n",
    "    (\"SK하이닉스\", \"SK Hynix\"),\n",
    "    (\"SK하이닉스\", \"하이닉스\"),\n",
    "    (\"네이버\", \"NAVER\"),\n",
    "    (\"네이버\", \"Naver\"),\n",
    "    (\"하이퍼클로바X\", \"HyperCLOVA X\"),\n",
    "    (\"엔비디아\", \"NVIDIA\"),\n",
    "    (\"삼성전자\", \"LG전자\"),  # 이건 다른 엔티티!\n",
    "]\n",
    "\n",
    "print(f\"{'쌍':<35} {'Levenshtein':>12} {'Jaro-Winkler':>13} {'Token Sort':>11}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for a, b in test_pairs:\n",
    "    lev = fuzz.ratio(a, b)\n",
    "    jaro = fuzz.WRatio(a, b)  # Weighted Ratio (다양한 방법 중 최선)\n",
    "    token_sort = fuzz.token_sort_ratio(a, b)\n",
    "    \n",
    "    pair_str = f\"{a} vs {b}\"\n",
    "    print(f\"{pair_str:<35} {lev:>11.1f} {jaro:>12.1f} {token_sort:>10.1f}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "f1a2b3c4"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 임계값 실험: 0.7, 0.8, 0.9\n",
    "def find_similar_pairs(names: list[str], threshold: float = 0.8) -> list[tuple]:\n",
    "    \"\"\"엔티티 이름 리스트에서 유사한 쌍을 찾습니다.\n",
    "    \n",
    "    Args:\n",
    "        names: 엔티티 이름 리스트\n",
    "        threshold: 유사도 임계값 (0~100)\n",
    "    \n",
    "    Returns:\n",
    "        (이름1, 이름2, 유사도) 튜플 리스트\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    for i, name_a in enumerate(names):\n",
    "        for j, name_b in enumerate(names):\n",
    "            if i >= j:  # 중복 방지\n",
    "                continue\n",
    "            \n",
    "            # 여러 유사도 메트릭 중 최대값 사용\n",
    "            score = max(\n",
    "                fuzz.ratio(name_a, name_b),\n",
    "                fuzz.WRatio(name_a, name_b),\n",
    "                fuzz.token_sort_ratio(name_a, name_b)\n",
    "            )\n",
    "            \n",
    "            if score >= threshold:\n",
    "                pairs.append((name_a, name_b, score))\n",
    "    \n",
    "    return sorted(pairs, key=lambda x: -x[2])\n",
    "\n",
    "# 임계값별 결과 비교\n",
    "for threshold in [70, 80, 90]:\n",
    "    pairs = find_similar_pairs(entity_names, threshold)\n",
    "    print(f\"\\n임계값 {threshold}% — 유사 쌍: {len(pairs)}개\")\n",
    "    for a, b, score in pairs:\n",
    "        print(f\"  [{score:.0f}%] {a} ≈ {b}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "d5e6f7a8"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 후보 쌍 생성 함수 (프로덕션 버전)\n",
    "def generate_candidate_pairs(\n",
    "    names: list[str], \n",
    "    threshold: float = 80,\n",
    "    method: str = \"combined\"\n",
    ") -> list[dict]:\n",
    "    \"\"\"Entity Resolution 후보 쌍을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        names: 엔티티 이름 리스트\n",
    "        threshold: 유사도 임계값\n",
    "        method: 유사도 계산 방법 (\"levenshtein\", \"jaro\", \"token\", \"combined\")\n",
    "    \n",
    "    Returns:\n",
    "        후보 쌍 딕셔너리 리스트\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    \n",
    "    for i, name_a in enumerate(names):\n",
    "        for j, name_b in enumerate(names):\n",
    "            if i >= j:\n",
    "                continue\n",
    "            \n",
    "            scores = {\n",
    "                \"levenshtein\": fuzz.ratio(name_a, name_b),\n",
    "                \"jaro\": fuzz.WRatio(name_a, name_b),\n",
    "                \"token_sort\": fuzz.token_sort_ratio(name_a, name_b),\n",
    "                \"partial\": fuzz.partial_ratio(name_a, name_b)\n",
    "            }\n",
    "            \n",
    "            if method == \"combined\":\n",
    "                final_score = max(scores.values())\n",
    "            else:\n",
    "                final_score = scores.get(method, 0)\n",
    "            \n",
    "            if final_score >= threshold:\n",
    "                candidates.append({\n",
    "                    \"entity_a\": name_a,\n",
    "                    \"entity_b\": name_b,\n",
    "                    \"score\": final_score,\n",
    "                    \"scores\": scores\n",
    "                })\n",
    "    \n",
    "    return sorted(candidates, key=lambda x: -x[\"score\"])\n",
    "\n",
    "# 임계값 80으로 후보 쌍 생성\n",
    "string_candidates = generate_candidate_pairs(entity_names, threshold=80)\n",
    "\n",
    "print(f\"문자열 유사도 기반 후보 쌍: {len(string_candidates)}개\")\n",
    "df_string = pd.DataFrame([\n",
    "    {\"엔티티 A\": c[\"entity_a\"], \"엔티티 B\": c[\"entity_b\"], \"최종 점수\": c[\"score\"],\n",
    "     **c[\"scores\"]}\n",
    "    for c in string_candidates\n",
    "])\n",
    "df_string"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "b9c0d1e2"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 관찰\n",
    "\n",
    "문자열 유사도의 **한계**:\n",
    "- \"삼성전자\" vs \"Samsung Electronics\" → 문자열이 전혀 달라서 낮은 점수\n",
    "- 한글-영문 변환을 감지하지 못함\n",
    "- 약어(Samsung → 삼성)를 이해하지 못함\n",
    "\n",
    "**해결:** 임베딩 기반 유사도로 **의미적 유사성**을 포착합니다."
   ],
   "id": "f3a4b5c6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. 임베딩 기반 Entity Resolution\n",
    "\n",
    "OpenAI Embedding 모델을 사용하여 엔티티 이름을 **벡터로 변환**하고,  \n",
    "**코사인 유사도**로 의미적으로 같은 엔티티를 찾습니다."
   ],
   "id": "d7e8f9a0"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def get_embeddings(texts: list[str], model: str = \"text-embedding-3-small\") -> np.ndarray:\n",
    "    \"\"\"텍스트 리스트의 임베딩을 생성합니다.\n",
    "    \n",
    "    Args:\n",
    "        texts: 텍스트 리스트\n",
    "        model: 임베딩 모델명\n",
    "    \n",
    "    Returns:\n",
    "        임베딩 행렬 (N x D)\n",
    "    \"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        input=texts,\n",
    "        model=model\n",
    "    )\n",
    "    \n",
    "    embeddings = [item.embedding for item in response.data]\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# 모든 엔티티 이름 임베딩\n",
    "print(f\"엔티티 {len(entity_names)}개 임베딩 생성 중...\")\n",
    "embeddings = get_embeddings(entity_names)\n",
    "print(f\"임베딩 행렬 크기: {embeddings.shape}\")\n",
    "print(f\"임베딩 차원: {embeddings.shape[1]}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "b1c2d3e5"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def cosine_similarity_matrix(embeddings: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"임베딩 행렬의 코사인 유사도 행렬을 계산합니다.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: (N, D) 임베딩 행렬\n",
    "    \n",
    "    Returns:\n",
    "        (N, N) 코사인 유사도 행렬\n",
    "    \"\"\"\n",
    "    # L2 정규화\n",
    "    norms = np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    normalized = embeddings / norms\n",
    "    \n",
    "    # 코사인 유사도 = 내적 (정규화 후)\n",
    "    similarity = normalized @ normalized.T\n",
    "    return similarity\n",
    "\n",
    "# 유사도 행렬 계산\n",
    "sim_matrix = cosine_similarity_matrix(embeddings)\n",
    "print(f\"유사도 행렬 크기: {sim_matrix.shape}\")\n",
    "\n",
    "# 히트맵 시각화\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "im = ax.imshow(sim_matrix, cmap=\"YlOrRd\", vmin=0.5, vmax=1.0)\n",
    "ax.set_xticks(range(len(entity_names)))\n",
    "ax.set_yticks(range(len(entity_names)))\n",
    "ax.set_xticklabels(entity_names, rotation=45, ha=\"right\", fontsize=8)\n",
    "ax.set_yticklabels(entity_names, fontsize=8)\n",
    "ax.set_title(\"엔티티 임베딩 코사인 유사도 히트맵\")\n",
    "plt.colorbar(im, ax=ax, label=\"코사인 유사도\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "f5a6b7c9"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def find_embedding_pairs(\n",
    "    names: list[str], \n",
    "    sim_matrix: np.ndarray, \n",
    "    threshold: float = 0.85\n",
    ") -> list[dict]:\n",
    "    \"\"\"임베딩 유사도 기반으로 후보 쌍을 찾습니다.\n",
    "    \n",
    "    Args:\n",
    "        names: 엔티티 이름 리스트\n",
    "        sim_matrix: 코사인 유사도 행렬\n",
    "        threshold: 유사도 임계값 (0~1)\n",
    "    \n",
    "    Returns:\n",
    "        후보 쌍 딕셔너리 리스트\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    n = len(names)\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            score = sim_matrix[i, j]\n",
    "            if score >= threshold:\n",
    "                pairs.append({\n",
    "                    \"entity_a\": names[i],\n",
    "                    \"entity_b\": names[j],\n",
    "                    \"cosine_similarity\": round(float(score), 4)\n",
    "                })\n",
    "    \n",
    "    return sorted(pairs, key=lambda x: -x[\"cosine_similarity\"])\n",
    "\n",
    "# 임베딩 기반 후보 쌍 (임계값 0.85)\n",
    "embedding_candidates = find_embedding_pairs(entity_names, sim_matrix, threshold=0.85)\n",
    "\n",
    "print(f\"임베딩 기반 후보 쌍: {len(embedding_candidates)}개\")\n",
    "for c in embedding_candidates:\n",
    "    print(f\"  [{c['cosine_similarity']:.4f}] {c['entity_a']} ≈ {c['entity_b']}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "d9e0f1a3"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 클러스터링으로 동일 엔티티 그룹화\n",
    "def cluster_entities(\n",
    "    names: list[str], \n",
    "    sim_matrix: np.ndarray, \n",
    "    threshold: float = 0.85\n",
    ") -> list[list[str]]:\n",
    "    \"\"\"유사한 엔티티를 클러스터로 그룹화합니다.\n",
    "    \n",
    "    단순 연결 컴포넌트 방식: A≈B, B≈C이면 {A,B,C}는 같은 그룹\n",
    "    \n",
    "    Args:\n",
    "        names: 엔티티 이름 리스트\n",
    "        sim_matrix: 코사인 유사도 행렬\n",
    "        threshold: 유사도 임계값\n",
    "    \n",
    "    Returns:\n",
    "        엔티티 이름 클러스터 리스트 (2개 이상인 그룹만)\n",
    "    \"\"\"\n",
    "    n = len(names)\n",
    "    # Union-Find\n",
    "    parent = list(range(n))\n",
    "    \n",
    "    def find(x):\n",
    "        while parent[x] != x:\n",
    "            parent[x] = parent[parent[x]]\n",
    "            x = parent[x]\n",
    "        return x\n",
    "    \n",
    "    def union(x, y):\n",
    "        px, py = find(x), find(y)\n",
    "        if px != py:\n",
    "            parent[px] = py\n",
    "    \n",
    "    # 임계값 이상인 쌍을 연결\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            if sim_matrix[i, j] >= threshold:\n",
    "                union(i, j)\n",
    "    \n",
    "    # 그룹 생성\n",
    "    groups = defaultdict(list)\n",
    "    for i in range(n):\n",
    "        groups[find(i)].append(names[i])\n",
    "    \n",
    "    # 2개 이상인 그룹만 반환\n",
    "    return [group for group in groups.values() if len(group) >= 2]\n",
    "\n",
    "# 클러스터링 실행\n",
    "clusters = cluster_entities(entity_names, sim_matrix, threshold=0.85)\n",
    "\n",
    "print(f\"동일 엔티티 클러스터: {len(clusters)}개 그룹\")\n",
    "for i, cluster in enumerate(clusters, 1):\n",
    "    canonical = cluster[0]  # 첫 번째를 대표 이름으로\n",
    "    print(f\"\\n  그룹 {i} (대표: {canonical}):\")\n",
    "    for name in cluster:\n",
    "        marker = \" [대표]\" if name == canonical else \"\"\n",
    "        print(f\"    - {name}{marker}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "b3c4d5e7"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 문자열 vs 임베딩 방식 비교\n",
    "print(\"방식 비교: 문자열 유사도 vs 임베딩 유사도\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 문자열 기반 결과\n",
    "string_pairs_set = {\n",
    "    (c[\"entity_a\"], c[\"entity_b\"]) for c in string_candidates\n",
    "}\n",
    "\n",
    "# 임베딩 기반 결과\n",
    "embedding_pairs_set = {\n",
    "    (c[\"entity_a\"], c[\"entity_b\"]) for c in embedding_candidates\n",
    "}\n",
    "\n",
    "# 비교\n",
    "only_string = string_pairs_set - embedding_pairs_set\n",
    "only_embedding = embedding_pairs_set - string_pairs_set\n",
    "both = string_pairs_set & embedding_pairs_set\n",
    "\n",
    "print(f\"\\n문자열만 감지: {len(only_string)}개\")\n",
    "for a, b in only_string:\n",
    "    print(f\"  {a} ≈ {b}\")\n",
    "\n",
    "print(f\"\\n임베딩만 감지: {len(only_embedding)}개\")\n",
    "for a, b in only_embedding:\n",
    "    print(f\"  {a} ≈ {b}\")\n",
    "\n",
    "print(f\"\\n양쪽 모두 감지: {len(both)}개\")\n",
    "for a, b in both:\n",
    "    print(f\"  {a} ≈ {b}\")\n",
    "\n",
    "print(f\"\\n결론:\")\n",
    "print(f\"- 임베딩은 한글-영문 변환을 잘 감지 (의미적 유사도)\")\n",
    "print(f\"- 문자열은 오타, 띄어쓰기 차이를 잘 감지 (형태적 유사도)\")\n",
    "print(f\"- 두 방식을 결합하면 최고의 결과를 얻을 수 있습니다\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "f7a8b9c1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Neo4j에서 노드 통합\n",
    "\n",
    "클러스터링 결과를 바탕으로 중복 노드를 **하나로 통합**합니다.\n",
    "\n",
    "통합 과정:\n",
    "1. 대표 노드 선정 (가장 한국어스러운 이름 / 가장 많은 관계를 가진 노드)\n",
    "2. 중복 노드의 속성을 대표 노드로 복사\n",
    "3. 중복 노드의 관계를 대표 노드로 재연결\n",
    "4. 중복 노드 삭제"
   ],
   "id": "d1e2f3a5"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def select_canonical_name(names: list[str]) -> str:\n",
    "    \"\"\"클러스터에서 대표 이름을 선정합니다.\n",
    "    \n",
    "    우선순위:\n",
    "    1. 한글 이름 (한국어 KG이므로)\n",
    "    2. 더 긴 이름 (더 공식적인 경향)\n",
    "    \n",
    "    Args:\n",
    "        names: 같은 엔티티의 이름 리스트\n",
    "    \n",
    "    Returns:\n",
    "        대표 이름\n",
    "    \"\"\"\n",
    "    def score(name):\n",
    "        # 한글 포함 여부 (높을수록 좋음)\n",
    "        has_korean = any('가' <= c <= '힣' for c in name)\n",
    "        korean_score = 100 if has_korean else 0\n",
    "        # 길이 (길수록 공식 명칭에 가까움)\n",
    "        length_score = len(name)\n",
    "        return korean_score + length_score\n",
    "    \n",
    "    return max(names, key=score)\n",
    "\n",
    "# 대표 이름 선정 테스트\n",
    "for cluster in clusters:\n",
    "    canonical = select_canonical_name(cluster)\n",
    "    others = [n for n in cluster if n != canonical]\n",
    "    print(f\"  대표: '{canonical}' ← 통합: {others}\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "b5c6d7e9"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def merge_entities_in_neo4j(\n",
    "    driver, \n",
    "    canonical_name: str, \n",
    "    duplicate_names: list[str]\n",
    ") -> dict:\n",
    "    \"\"\"Neo4j에서 중복 엔티티를 대표 엔티티로 통합합니다.\n",
    "    \n",
    "    Args:\n",
    "        driver: Neo4j 드라이버\n",
    "        canonical_name: 대표 엔티티 이름\n",
    "        duplicate_names: 통합될 중복 엔티티 이름 리스트\n",
    "    \n",
    "    Returns:\n",
    "        통합 결과 딕셔너리\n",
    "    \"\"\"\n",
    "    stats = {\"relations_moved\": 0, \"nodes_deleted\": 0, \"properties_merged\": 0}\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        for dup_name in duplicate_names:\n",
    "            if dup_name == canonical_name:\n",
    "                continue\n",
    "            \n",
    "            # 1. 중복 노드의 속성을 대표 노드로 복사\n",
    "            session.run(\"\"\"\n",
    "                MATCH (canonical {name: $canonical_name})\n",
    "                MATCH (dup {name: $dup_name})\n",
    "                SET canonical += properties(dup)\n",
    "                SET canonical.name = $canonical_name\n",
    "                SET canonical.aliases = coalesce(canonical.aliases, []) + [$dup_name]\n",
    "            \"\"\", canonical_name=canonical_name, dup_name=dup_name)\n",
    "            stats[\"properties_merged\"] += 1\n",
    "            \n",
    "            # 2. 나가는 관계 재연결 (dup → X 를 canonical → X 로)\n",
    "            result = session.run(\"\"\"\n",
    "                MATCH (dup {name: $dup_name})-[r]->(target)\n",
    "                MATCH (canonical {name: $canonical_name})\n",
    "                WITH canonical, target, type(r) AS relType, properties(r) AS props\n",
    "                CALL apoc.create.relationship(canonical, relType, props, target) YIELD rel\n",
    "                RETURN count(rel) AS cnt\n",
    "            \"\"\", dup_name=dup_name, canonical_name=canonical_name)\n",
    "            cnt = result.single()[\"cnt\"]\n",
    "            stats[\"relations_moved\"] += cnt\n",
    "            \n",
    "            # 3. 들어오는 관계 재연결 (X → dup 를 X → canonical 로)\n",
    "            result = session.run(\"\"\"\n",
    "                MATCH (source)-[r]->(dup {name: $dup_name})\n",
    "                MATCH (canonical {name: $canonical_name})\n",
    "                WITH source, canonical, type(r) AS relType, properties(r) AS props\n",
    "                CALL apoc.create.relationship(source, relType, props, canonical) YIELD rel\n",
    "                RETURN count(rel) AS cnt\n",
    "            \"\"\", dup_name=dup_name, canonical_name=canonical_name)\n",
    "            cnt = result.single()[\"cnt\"]\n",
    "            stats[\"relations_moved\"] += cnt\n",
    "            \n",
    "            # 4. 중복 노드 삭제\n",
    "            session.run(\"\"\"\n",
    "                MATCH (dup {name: $dup_name})\n",
    "                DETACH DELETE dup\n",
    "            \"\"\", dup_name=dup_name)\n",
    "            stats[\"nodes_deleted\"] += 1\n",
    "    \n",
    "    return stats\n",
    "\n",
    "print(\"통합 함수 정의 완료\")\n",
    "print(\"주의: 이 함수는 APOC 플러그인이 필요합니다.\")\n",
    "print(\"APOC 없이 실행하려면 아래의 대체 함수를 사용하세요.\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "f9a0b1c3"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# APOC 없이 사용 가능한 대체 함수\n",
    "def merge_entities_basic(\n",
    "    driver, \n",
    "    canonical_name: str, \n",
    "    duplicate_names: list[str]\n",
    ") -> dict:\n",
    "    \"\"\"APOC 없이 순수 Cypher로 엔티티를 통합합니다.\n",
    "    \n",
    "    제약: 관계 타입을 동적으로 생성할 수 없으므로,\n",
    "    알려진 관계 타입을 명시적으로 처리합니다.\n",
    "    \"\"\"\n",
    "    KNOWN_REL_TYPES = [\n",
    "        \"DEVELOPS\", \"INVESTS_IN\", \"COMPETES_WITH\", \"PARTNERS_WITH\",\n",
    "        \"LEADS\", \"LOCATED_AT\", \"SUPPLIES_TO\", \"USES\"\n",
    "    ]\n",
    "    \n",
    "    stats = {\"relations_moved\": 0, \"nodes_deleted\": 0}\n",
    "    \n",
    "    with driver.session() as session:\n",
    "        for dup_name in duplicate_names:\n",
    "            if dup_name == canonical_name:\n",
    "                continue\n",
    "            \n",
    "            # 속성 복사 + aliases 추가\n",
    "            session.run(\"\"\"\n",
    "                MATCH (canonical {name: $canonical_name})\n",
    "                MATCH (dup {name: $dup_name})\n",
    "                SET canonical += properties(dup)\n",
    "                SET canonical.name = $canonical_name\n",
    "                SET canonical.aliases = coalesce(canonical.aliases, []) + [$dup_name]\n",
    "            \"\"\", canonical_name=canonical_name, dup_name=dup_name)\n",
    "            \n",
    "            # 각 관계 타입별로 재연결\n",
    "            for rel_type in KNOWN_REL_TYPES:\n",
    "                # 나가는 관계\n",
    "                result = session.run(f\"\"\"\n",
    "                    MATCH (dup {{name: $dup_name}})-[r:{rel_type}]->(target)\n",
    "                    MATCH (canonical {{name: $canonical_name}})\n",
    "                    MERGE (canonical)-[:{rel_type}]->(target)\n",
    "                    DELETE r\n",
    "                    RETURN count(r) AS cnt\n",
    "                \"\"\", dup_name=dup_name, canonical_name=canonical_name)\n",
    "                stats[\"relations_moved\"] += result.single()[\"cnt\"]\n",
    "                \n",
    "                # 들어오는 관계\n",
    "                result = session.run(f\"\"\"\n",
    "                    MATCH (source)-[r:{rel_type}]->(dup {{name: $dup_name}})\n",
    "                    MATCH (canonical {{name: $canonical_name}})\n",
    "                    MERGE (source)-[:{rel_type}]->(canonical)\n",
    "                    DELETE r\n",
    "                    RETURN count(r) AS cnt\n",
    "                \"\"\", dup_name=dup_name, canonical_name=canonical_name)\n",
    "                stats[\"relations_moved\"] += result.single()[\"cnt\"]\n",
    "            \n",
    "            # 중복 노드 삭제\n",
    "            session.run(\"\"\"\n",
    "                MATCH (dup {name: $dup_name})\n",
    "                DETACH DELETE dup\n",
    "            \"\"\", dup_name=dup_name)\n",
    "            stats[\"nodes_deleted\"] += 1\n",
    "    \n",
    "    return stats\n",
    "\n",
    "print(\"기본 통합 함수(APOC 불필요) 정의 완료\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "d3e4f5a7"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 통합 전 상태 기록\n",
    "with driver.session() as session:\n",
    "    before_nodes = session.run(\"MATCH (n) RETURN count(n) AS cnt\").single()[\"cnt\"]\n",
    "    before_rels = session.run(\"MATCH ()-[r]->() RETURN count(r) AS cnt\").single()[\"cnt\"]\n",
    "\n",
    "print(f\"통합 전 상태:\")\n",
    "print(f\"  노드: {before_nodes}개\")\n",
    "print(f\"  관계: {before_rels}개\")\n",
    "\n",
    "# 클러스터별 통합 실행\n",
    "total_stats = {\"relations_moved\": 0, \"nodes_deleted\": 0}\n",
    "\n",
    "for cluster in clusters:\n",
    "    canonical = select_canonical_name(cluster)\n",
    "    duplicates = [n for n in cluster if n != canonical]\n",
    "    \n",
    "    if not duplicates:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n통합: '{canonical}' ← {duplicates}\")\n",
    "    stats = merge_entities_basic(driver, canonical, duplicates)\n",
    "    \n",
    "    total_stats[\"relations_moved\"] += stats[\"relations_moved\"]\n",
    "    total_stats[\"nodes_deleted\"] += stats[\"nodes_deleted\"]\n",
    "    \n",
    "    print(f\"  삭제된 노드: {stats['nodes_deleted']}개\")\n",
    "    print(f\"  이동된 관계: {stats['relations_moved']}개\")\n",
    "\n",
    "# 통합 후 상태\n",
    "with driver.session() as session:\n",
    "    after_nodes = session.run(\"MATCH (n) RETURN count(n) AS cnt\").single()[\"cnt\"]\n",
    "    after_rels = session.run(\"MATCH ()-[r]->() RETURN count(r) AS cnt\").single()[\"cnt\"]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"통합 결과 요약:\")\n",
    "print(f\"  노드: {before_nodes}개 → {after_nodes}개 (△{after_nodes - before_nodes})\")\n",
    "print(f\"  관계: {before_rels}개 → {after_rels}개 (△{after_rels - before_rels})\")\n",
    "print(f\"  삭제된 중복 노드: {total_stats['nodes_deleted']}개\")\n",
    "print(f\"  이동된 관계: {total_stats['relations_moved']}개\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "b7c8d9e1"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 통합 후 Multi-hop 쿼리 재실행 → 결과 개선 확인\n",
    "with driver.session() as session:\n",
    "    # 이제 \"삼성전자\"로 검색하면 Samsung Electronics의 관계도 포함\n",
    "    result_after = session.run(\"\"\"\n",
    "        MATCH (samsung:Company {name: \"삼성전자\"})-[r]-(connected)\n",
    "        RETURN samsung.name AS source, type(r) AS relation, connected.name AS target\n",
    "    \"\"\").data()\n",
    "    \n",
    "    # aliases 확인\n",
    "    aliases = session.run(\"\"\"\n",
    "        MATCH (n)\n",
    "        WHERE n.aliases IS NOT NULL AND size(n.aliases) > 0\n",
    "        RETURN n.name AS name, n.aliases AS aliases\n",
    "    \"\"\").data()\n",
    "    \n",
    "    # 2-hop 연결 재실행\n",
    "    result_2hop = session.run(\"\"\"\n",
    "        MATCH path = (samsung:Company {name: \"삼성전자\"})-[*1..2]-(target)\n",
    "        RETURN DISTINCT target.name AS reachable\n",
    "    \"\"\").data()\n",
    "\n",
    "print(\"통합 후 쿼리 결과:\")\n",
    "print(f\"\\n1) '삼성전자' 관계 (통합 후): {len(result_after)}개\")\n",
    "for r in result_after:\n",
    "    print(f\"   {r['source']} --[{r['relation']}]--> {r['target']}\")\n",
    "\n",
    "print(f\"\\n2) aliases가 있는 엔티티:\")\n",
    "for a in aliases:\n",
    "    print(f\"   {a['name']}: {a['aliases']}\")\n",
    "\n",
    "print(f\"\\n3) '삼성전자'에서 2-hop 도달 가능 (통합 후): {len(result_2hop)}개\")\n",
    "for r in result_2hop:\n",
    "    print(f\"   - {r['reachable']}\")\n",
    "\n",
    "print(f\"\\n이전에 'Samsung Electronics'로만 접근 가능했던 관계가\")\n",
    "print(f\"이제 '삼성전자'로도 접근 가능합니다!\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "f1a2b3c5"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 통합 전/후 비교 시각화\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# 노드 수 비교\n",
    "categories = [\"통합 전\", \"통합 후\"]\n",
    "node_values = [before_nodes, after_nodes]\n",
    "rel_values = [before_rels, after_rels]\n",
    "\n",
    "ax1 = axes[0]\n",
    "bars = ax1.bar(categories, node_values, color=[\"#ef4444\", \"#22c55e\"], width=0.5)\n",
    "ax1.set_ylabel(\"노드 수\")\n",
    "ax1.set_title(\"노드 수 변화\")\n",
    "for bar, val in zip(bars, node_values):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n",
    "             str(val), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 관계 수 비교\n",
    "ax2 = axes[1]\n",
    "bars = ax2.bar(categories, rel_values, color=[\"#ef4444\", \"#22c55e\"], width=0.5)\n",
    "ax2.set_ylabel(\"관계 수\")\n",
    "ax2.set_title(\"관계 수 변화\")\n",
    "for bar, val in zip(bars, rel_values):\n",
    "    ax2.text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.5,\n",
    "             str(val), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.suptitle(\"Entity Resolution 전/후 비교\", fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "reduction = ((before_nodes - after_nodes) / before_nodes * 100) if before_nodes > 0 else 0\n",
    "print(f\"\\n노드 감소율: {reduction:.1f}%\")\n",
    "print(f\"중복 제거로 그래프가 더 정확하고 효율적으로 변했습니다.\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "d5e6f7a9"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. 연습 문제\n",
    "\n",
    "### 연습 6.1: 제조 도메인에서 중복 장비명 통합\n",
    "\n",
    "제조 도메인 데이터에는 같은 장비를 다르게 부르는 경우가 흔합니다:  \n",
    "- \"MX-100\" vs \"원료혼합기\" vs \"MX100\"\n",
    "- \"PR-200\" vs \"프레스기\" vs \"PR200\"\n",
    "\n",
    "아래 코드를 완성하여 제조 도메인의 중복 엔티티를 통합해보세요."
   ],
   "id": "b9c0d1e3"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 연습 6.1: 제조 도메인 중복 통합\n",
    "\n",
    "# 제조 도메인 엔티티 (실제로는 Part 3 연습에서 추출한 결과 사용)\n",
    "manufacturing_entities = [\n",
    "    \"MX-100\", \"원료혼합기\", \"MX100\",\n",
    "    \"PR-200\", \"프레스기\", \"PR200\",\n",
    "    \"HT-300\", \"열처리로\", \"HT300\",\n",
    "    \"GR-400\", \"연마기\", \"GR400\",\n",
    "    \"QC-500\", \"검사장비\", \"QC500\",\n",
    "    \"삼성정밀\", \"Samsung Precision\",\n",
    "    \"현대위아\", \"Hyundai Wia\",\n",
    "    \"포스코DX\", \"POSCO DX\", \"포스코디엑스\"\n",
    "]\n",
    "\n",
    "# TODO: 임베딩 기반으로 중복 감지\n",
    "print(\"제조 도메인 엔티티 임베딩 생성 중...\")\n",
    "mfg_embeddings = get_embeddings(manufacturing_entities)\n",
    "mfg_sim_matrix = cosine_similarity_matrix(mfg_embeddings)\n",
    "\n",
    "# TODO: 클러스터링\n",
    "mfg_clusters = cluster_entities(manufacturing_entities, mfg_sim_matrix, threshold=0.80)\n",
    "\n",
    "print(f\"\\n제조 도메인 동일 엔티티 클러스터: {len(mfg_clusters)}개\")\n",
    "for i, cluster in enumerate(mfg_clusters, 1):\n",
    "    canonical = select_canonical_name(cluster)\n",
    "    print(f\"  그룹 {i}: 대표 '{canonical}' ← {[n for n in cluster if n != canonical]}\")\n",
    "\n",
    "# TODO: 임계값을 조정하여 최적 결과를 찾아보세요 (0.75, 0.80, 0.85, 0.90)"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "f3a4b5c7"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 연습 6.2: 최적 임계값 찾기\n",
    "\n",
    "임계값이 너무 낮으면 다른 엔티티를 통합하고 (False Positive),  \n",
    "너무 높으면 같은 엔티티를 놓칩니다 (False Negative).\n",
    "\n",
    "임계값별 클러스터 수를 비교하여 최적값을 찾아보세요."
   ],
   "id": "d7e8f9a1"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 연습 6.2: 임계값 최적화\n",
    "thresholds = [0.70, 0.75, 0.80, 0.85, 0.90, 0.95]\n",
    "threshold_results = []\n",
    "\n",
    "for t in thresholds:\n",
    "    clusters_t = cluster_entities(entity_names, sim_matrix, threshold=t)\n",
    "    n_clusters = len(clusters_t)\n",
    "    n_merged = sum(len(c) - 1 for c in clusters_t)  # 통합될 노드 수\n",
    "    \n",
    "    threshold_results.append({\n",
    "        \"임계값\": t,\n",
    "        \"클러스터 수\": n_clusters,\n",
    "        \"통합될 노드\": n_merged,\n",
    "        \"잔여 노드\": len(entity_names) - n_merged\n",
    "    })\n",
    "\n",
    "df_thresholds = pd.DataFrame(threshold_results)\n",
    "print(\"임계값별 결과:\")\n",
    "print(df_thresholds.to_string(index=False))\n",
    "\n",
    "# 시각화\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.plot(thresholds, [r[\"클러스터 수\"] for r in threshold_results], \n",
    "        marker='o', label=\"클러스터 수\", color=\"#3b82f6\")\n",
    "ax.plot(thresholds, [r[\"통합될 노드\"] for r in threshold_results], \n",
    "        marker='s', label=\"통합될 노드 수\", color=\"#ef4444\")\n",
    "\n",
    "ax.set_xlabel(\"코사인 유사도 임계값\")\n",
    "ax.set_ylabel(\"개수\")\n",
    "ax.set_title(\"임계값에 따른 Entity Resolution 결과\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 최적 임계값 표시 (팔꿈치 지점)\n",
    "ax.axvline(x=0.85, color='gray', linestyle='--', alpha=0.5, label=\"추천 임계값\")\n",
    "ax.annotate('추천: 0.85', xy=(0.85, max(r[\"클러스터 수\"] for r in threshold_results)),\n",
    "            fontsize=10, ha='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n일반적으로 0.85가 좋은 출발점입니다.\")\n",
    "print(\"도메인과 데이터에 따라 조정이 필요합니다.\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "b1c2d3e6"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 정리\n",
    "\n",
    "### 이번 파트에서 배운 것\n",
    "\n",
    "1. **중복 엔티티는 KG 품질을 심각하게 저하**시킨다 (특히 Multi-hop 쿼리)\n",
    "2. **문자열 유사도**는 오타/띄어쓰기에 강하지만, 한글-영문 변환에 약하다\n",
    "3. **임베딩 기반 유사도**는 의미적 동일성을 포착한다 (한글-영문 OK)\n",
    "4. **두 방식을 결합**하면 최고의 ER 성능을 얻을 수 있다\n",
    "5. **임계값 튜닝**이 중요하다 (너무 낮으면 False Positive, 너무 높으면 False Negative)\n",
    "6. **aliases 필드**로 통합 이력을 보존하는 것이 좋다\n",
    "\n",
    "### 다음 파트 예고\n",
    "\n",
    "**Part 5: 멀티모달 VLM** - 표와 이미지가 포함된 실무 문서를 그래프로 변환합니다."
   ],
   "id": "f5a6b7c0"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# 리소스 정리\n",
    "driver.close()\n",
    "print(\"Neo4j 연결 종료\")\n",
    "print(\"Part 4 실습 완료!\")"
   ],
   "execution_count": null,
   "outputs": [],
   "id": "d9e0f1a4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
